{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LXML\n",
    "\n",
    "LXML과 Open API를 활용하여 판례를 로컬폴더에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml_OpenAPI import all_in_one as do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do('손해배상', 3000, 'case_name', '사건명')\n",
    "do('손해배상', 3000, 'case_summary', '판시사항')\n",
    "do('손해배상', 3000, 'case_judgement', '판결요지')\n",
    "do('손해배상', 3000, 'case_content', '판례내용')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencepiece\n",
    "\n",
    "parameter로 Vocab size를 받아서 LXML로 저장한 판례 데이터를 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zG1eJvR51v-l",
    "outputId": "59009a8e-fc6c-4d07-e91d-4387932d2e58"
   },
   "outputs": [],
   "source": [
    "import sentencepiece_on_project as sop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-Q0Nqw228SJ"
   },
   "outputs": [],
   "source": [
    "sop.train_sp(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M5fREpCZ6xEa",
    "outputId": "f5b9618a-2d96-4221-e149-a5d3c617f25d"
   },
   "outputs": [],
   "source": [
    "sentencepiece = sop.get_sentencepiece()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF 알고리즘을 이용하여 로컬파일들을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn 패키지를 이용하여 tfidf를 구현합니다.\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn을 이용하여 tfidf를 구현하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시용 import 입니다.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jA2pJznlq1_r"
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "'[1] 일반육체노동을 하는 사람 또는 육체노동을 주로 생계활동으로 하는 사람의 가동연한을 경험칙상 만 65세까지로 보아야 하는지 여부(원칙적 적극)',\n",
    "'[2] 일실수입 산정의 기초가 되는 피해자의 가동연한을 인정하는 기준',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Z2-AmQjXtl_s",
    "outputId": "e60176b2-824a-4db3-db19-ab50749bf076"
   },
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "o-3EXZmE1O-l",
    "outputId": "fd7af4a4-87f4-484c-b736-50005b484e3c"
   },
   "outputs": [],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "s = test_data[0].replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dupfFES216aZ"
   },
   "outputs": [],
   "source": [
    "# 불용어 제거를 위해 불용어 리스트를 작성합니다.\n",
    "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "turUjpSYtv7W"
   },
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "for sentence in test_data:\n",
    "    temp_X = sentencepiece.EncodeAsPieces(sentence) # 학습된 sentenpiece 모델을 이용합니다.\n",
    "    temp_X=[word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sQQGSMy9uIDv",
    "outputId": "aaad2f28-d77e-4775-ecdf-ecb271e3c8e0"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wz9Em-rLuqNM"
   },
   "outputs": [],
   "source": [
    "temp = ' '.join(X_train[0])\n",
    "print(temp)\n",
    "train_test=re.sub('▁', '', temp)\n",
    "train_test=re.sub('  ', ' ', train_test)\n",
    "print(train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로젝트상 실제 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz0fmkrtUgPW"
   },
   "outputs": [],
   "source": [
    "import tfidf\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거를 위해 불용어 리스트를 작성합니다.\n",
    "stopwords=['_', '의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XV3sNkPSW9KT"
   },
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    file_list = os.listdir(path)\n",
    "    for file in file_list:\n",
    "        if 'total' in file:\n",
    "            file_list.remove(file)\n",
    "    temp=[]\n",
    "    for f in file_list:\n",
    "        temp.append(int(f[:-4]))\n",
    "    temp = sorted(temp)\n",
    "    file_list=[]\n",
    "    \n",
    "    for i in range(len(temp)):\n",
    "        file_list.append(str(temp[i])+'.txt')\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        with open(path + '\\\\' + file_list[i], 'r') as data: \n",
    "            files.append(data.read())\n",
    "    return file_list, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'case_name')\n",
    "\n",
    "file_list, files = get_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "for file in files[:500]:\n",
    "    temp_X = sentencepiece.EncodeAsPieces(file) # 학습된 sentenpiece 모델을 이용합니다.\n",
    "    \n",
    "    temp_X=[word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "#     print(temp_X)\n",
    "    X_train.append(temp_X)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[500:1000]:\n",
    "    temp_X = sentencepiece.EncodeAsPieces(file) # 학습된 sentenpiece 모델을 이용합니다.\n",
    "    \n",
    "    temp_X=[word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "#     print(temp_X)\n",
    "    X_train.append(temp_X)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[1000:1500]:\n",
    "    temp_X = sentencepiece.EncodeAsPieces(file) # 학습된 sentenpiece 모델을 이용합니다.\n",
    "    \n",
    "    temp_X=[word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "#     print(temp_X)\n",
    "    X_train.append(temp_X)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[1500:]:\n",
    "    temp_X = sentencepiece.EncodeAsPieces(file) # 학습된 sentenpiece 모델을 이용합니다.\n",
    "    \n",
    "    temp_X=[word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "#     print(temp_X)\n",
    "    X_train.append(temp_X)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata = []\n",
    "for i in range(len(X_train)):\n",
    "    temp = ' '.join(X_train[i])\n",
    "    temp = re.sub('▁', '', temp)\n",
    "    realdata.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 train시키고 불러온 sentencepiece model을 이용하여 데이터를 전처리합니다.\n",
    "# jupyter notebook 에서 메모리 초과 오류가 흔히 발생하여 위의 예시처럼 500개씩 나눠서 전처리를 진행합니다.\n",
    "# data = tfidf.preprocessing(sentencepiece, files, stopwords, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터를 이용하여 tfidf를 모델을 학습시킵니다.\n",
    "model = tfidf.train(realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 tfidf모델을 저장합니다.\n",
    "tfidf.save(model, 'judgement_tfidf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 학습된 tfidf모델을 가져옵니다.\n",
    "# tfidf.save함수로 저장된 파일의 경우 .pk 확장자로 저장됨을 유의합니다.\n",
    "model = tfidf.load('tfidf_model.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DProjects\\main\\venv\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\DProjects\\main\\venv\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tfidf.load('tfidf_model.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b46b0b407771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mvocab_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mvocab_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_str\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('tfidf_vocab.txt', 'w+', encoding='utf-8') as vocab:\n",
    "    names = model.get_feature_names()\n",
    "    vocab_str=''\n",
    "    for name in names:\n",
    "        vocab_str = vocab_str+name+' '\n",
    "    vocab.write(vocab_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.save(names, 'tfidf_vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf의 결과를 판례일련번호로 json파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "import codecs\n",
    "import tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"tfidf_results\" ## your path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    file_list = os.listdir(path)\n",
    "    for file in file_list:\n",
    "        if 'total' in file:\n",
    "            file_list.remove(file)\n",
    "    temp=[]\n",
    "    for f in file_list:\n",
    "        temp.append(int(f[:-4]))\n",
    "    temp = sorted(temp)\n",
    "    file_list=[]\n",
    "    \n",
    "    for i in range(len(temp)):\n",
    "        file_list.append(str(temp[i])+'.txt')\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        with open(path + '\\\\' + file_list[i], 'r') as data: \n",
    "            files.append(data.read())\n",
    "    return file_list, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json으로 변환하기 위한 python dict의 틀을 잡습니다.\n",
    "def init_dict(_from, _to):\n",
    "    results = {}\n",
    "\n",
    "    path = os.path.join(os.getcwd(), 'case_name')\n",
    "    file_list, files = get_files(path)\n",
    "\n",
    "    for idx in range(_from, _to):\n",
    "        id = file_list[idx][:-4]\n",
    "        results[id] = {}\n",
    "        results[id]['precedent'] = {}\n",
    "        results[id]['precedent']['id'] = id\n",
    "        results[id]['summary'] = {}\n",
    "        results[id]['summary']['id'] = id\n",
    "        results[id]['judgement'] = {}\n",
    "        results[id]['judgement']['id'] = id\n",
    "        results[id]['content'] = {}\n",
    "        results[id]['content']['id'] = id\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tfidf_data(dict_on, DB_diagram, train_from, train_to):\n",
    "    # DB_diagram 내용을 tfidf를 돌린후\n",
    "    # DB의 다이어그램에 채울 데이터를 json으로 만들기 위해 dict에 저장합니다.\n",
    "    # 먼저 미리 저장해놓은 summary용 tfidf모델을 가져옵니다.\n",
    "    model = tfidf.load(DB_diagram + '_tfidf_model.pk')\n",
    "    path = os.path.join(os.getcwd(), 'case_' + DB_diagram)\n",
    "    file_list, files = get_files(path)\n",
    "    list_temp = file_list[train_from:train_to]\n",
    "    file_temp = files[train_from:train_to]\n",
    "    key_weights = tfidf.do(model, file_temp)\n",
    "\n",
    "\n",
    "    for idx in range(len(file_temp)):\n",
    "        id = list_temp[idx][:-4]\n",
    "        temp = []\n",
    "        for jdx in range(0, key_weights.shape[1]):\n",
    "            temp.append(jdx)\n",
    "        dict_on[id][DB_diagram]['key_weight'] = key_weights[idx]            \n",
    "        dict_on[id][DB_diagram]['key_index'] = np.array(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB의 precedent 다이어그램에 채울 데이터를 json으로 만들기 위해 dict에 저장합니다.\n",
    "def insert_name(results, a, b):\n",
    "    path = os.path.join(os.getcwd(), 'case_name')\n",
    "    file_list, files = get_files(path)\n",
    "\n",
    "    for idx in range(a, b):\n",
    "        results[file_list[idx][:-4]]['precedent']['casename'] = files[idx][:-1]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict에 data를 넣습니다.\n",
    "def sss(a, b):\n",
    "    results = init_dict(a, b)\n",
    "    results = insert_name(results, a, b)\n",
    "    set_tfidf_data(results, 'summary', a, b)\n",
    "    set_tfidf_data(results, 'judgement', a, b)\n",
    "    set_tfidf_data(results, 'content', a, b)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ID, trash = get_files(os.path.join(os.getcwd(), 'case_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207140succeed\n",
      "207485succeed\n",
      "207486succeed\n",
      "207490succeed\n",
      "207529succeed\n",
      "207541succeed\n",
      "207579succeed\n",
      "207580succeed\n",
      "207581succeed\n",
      "207588succeed\n",
      "207594succeed\n",
      "208415succeed\n",
      "208726succeed\n",
      "208733succeed\n",
      "208734succeed\n",
      "208744succeed\n",
      "208745succeed\n",
      "208795succeed\n",
      "208800succeed\n",
      "208848succeed\n",
      "208849succeed\n",
      "209131succeed\n",
      "209150succeed\n",
      "209160succeed\n",
      "209217succeed\n",
      "209252succeed\n",
      "209581succeed\n",
      "209587succeed\n",
      "209687succeed\n",
      "209693succeed\n",
      "209711succeed\n",
      "209715succeed\n",
      "209727succeed\n",
      "209729succeed\n",
      "209731succeed\n",
      "209747succeed\n",
      "209751succeed\n",
      "209785succeed\n",
      "209793succeed\n",
      "209859succeed\n",
      "210335succeed\n",
      "210415succeed\n",
      "210419succeed\n",
      "210423succeed\n",
      "210569succeed\n",
      "210767succeed\n",
      "210813succeed\n",
      "210827succeed\n",
      "210829succeed\n",
      "210853succeed\n",
      "210895succeed\n"
     ]
    }
   ],
   "source": [
    "a = 1800\n",
    "b = a + 51\n",
    "results = sss(a, b)\n",
    "dir_path = os.path.join(os.getcwd(), 'tfidf_results\\\\')\n",
    "for i in range(a, b):\n",
    "    # 앞전에서 받아놨던 일련번호를 개행문자를 제거하고 파일이름에 넣어줍니다.\n",
    "    file_path = os.path.join(dir_path, case_ID[i][:-4] + '.json')\n",
    "    with codecs.open(file_path, 'w+', encoding='utf-8') as json_file:\n",
    "        json.dump(results[case_ID[i][:-4]], json_file, separators=(',', ':'), \n",
    "                  sort_keys=True, indent=4, cls=NumpyEncoder)\n",
    "    print(case_ID[i][:-4] + 'succeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook에서 tfdif 분석결과를 확인하는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "for result in results:\n",
    "    for tfidf in result:\n",
    "        if tfidf != 0:\n",
    "            print (str(i) +\" \"+ str(j) + \" \"+ str(tfidf) )\n",
    "        j+=1\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sentencePiece+tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
